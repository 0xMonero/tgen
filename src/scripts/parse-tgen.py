#!/usr/bin/python

import sys, os, argparse, re

DESCRIPTION="""
A utility to help parse results from the tgen traffic generator.

This script enables processing of tgen log files and storing processed
data in json format for plotting. It was written so that the log files
need never be stored on disk decompressed, which is useful when log file
sizes reach tens of gigabytes.

Use the help menu to understand usage:
$ python parse-tgen.py -h

The standard way to run the script is to give the path to a directory tree
under which one or several tgen log files exist:
$ python parse-tgen.py shadow.data/hosts/
$ python parse-tgen.py ./

This path will be searched for log files whose names match those created
by shadow; additional patterns can be added with the '-e' option.

A single tgen log file can also be passed on STDIN with the special '-' path:
$ cat tgen.log | python parse-tgen.py -
$ xzcat tgen.log.xz | python parse-tgen.py -

The default mode is to filter and parse the log files using a single
process; this will be done with multiple worker processes when passing
the '-m' option.
"""

TGENJSON="stats.tgen.json"

def main():
    parser = argparse.ArgumentParser(
        description=DESCRIPTION, 
        formatter_class=argparse.RawTextHelpFormatter)#ArgumentDefaultsHelpFormatter)

    parser.add_argument(
        help="""The PATH to search for tgen log files, which may be '-'
for STDIN; each log file may end in '.xz' to enable
inline xz decompression""", 
        metavar="PATH",
        action="store", dest="searchpath")

    parser.add_argument('-e', '--expression',
        help="""Append a regex PATTERN to the list of strings used with
re.search to find tgen log file names in the search path""", 
        action="append", dest="patterns",
        metavar="PATTERN",
        default=["tgen.*\.log"])

    parser.add_argument('-m', '--multiproc',
        help="""Enable multiprocessing""", 
        action="store_true", dest="multiproc",
        default=False)

    parser.add_argument('-p', '--prefix', 
        help="""A STRING directory path prefix where the processed data
files generated by this script will be written""", 
        metavar="STRING",
        action="store", dest="prefix",
        default=os.getcwd())

    parser.add_argument('-s', '--skip',
        help="""Ignore the first N seconds of each log file while parsing""", 
        metavar="N",
        action="store", dest="skiptime", type=int,
        default=0)

    args = parser.parse_args()
    args.searchpath = os.path.abspath(os.path.expanduser(args.searchpath))
    args.prefix = os.path.abspath(os.path.expanduser(args.prefix))

    args.logfilepaths = find_file_paths(args.searchpath, args.patterns)
    go(args)

def go(args):
    print >> sys.stderr, "processing input from {0} files...".format(len(args.logfilepaths))
    success_count, error_count = 0, 0
    for p in args.logfilepaths:
        with open(p, 'r') as fin:
            for line in fin:
                if re.search("transfer-complete", line) is not None: success_count += 1
                if re.search("transfer-error", line) is not None: error_count += 1
    print >> sys.stderr, "done: {0} total successes, {1} total erorrs".format(success_count, error_count)

def find_file_paths(searchpath, patterns):
    paths = []
    if searchpath.endswith("/-"): paths.append("-")
    else:
        for root, dirs, files in os.walk(searchpath):
            for name in files:
                found = False
                fpath = os.path.join(root, name)
                fbase = os.path.basename(fpath)
                for pattern in patterns:
                    if re.search(pattern, fbase): found = True
                if found: paths.append(fpath)
    return paths

if __name__ == '__main__': sys.exit(main())

